{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence BERT 연구 코드\n",
    "- 최종 수정일 : 2025.03.14\n",
    "- 해당 코드는 Jupyter Notebook으로 실행이 가능합니다. Python 파일과는 달리 셀 별로 실행이 가능하다는 장점이 있습니다.\n",
    "- Python은 3.11.11 버전에서 실행 확인되었습니다.\n",
    "\n",
    "#### 적용 기술\n",
    "- 해당 코드를 실행하는 데 필요한 Python Module 설치 셀 생성\n",
    "- 문서 전처리 함수 정의\n",
    "- 문서 전처리 자동화 구현\n",
    "- 문서 평가 결과 자동 출력\n",
    "\n",
    "#### 업데이트 예정\n",
    "- 문서 전처리 함수 추가\n",
    "- 벡터 데이터베이스 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Modules - Just Execute Once!\n",
    "!pip install -r ./requirement.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Modules\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os, pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문서 전처리 함수 정의\n",
    "\n",
    "# 괄호 제거 함수\n",
    "def remove_parenthesse_func(text):\n",
    "    return re.sub(r\"[\\(\\{\\[].*?[\\)\\}\\]]\", \"\", text)\n",
    "\n",
    "# 문장 단위 분리 함수\n",
    "def split_sentence(text):\n",
    "    sentences = text.split(\". \")\n",
    "    return [sentence.strip().rstrip('.') for sentence in sentences if len(sentence) > 0 and sentence != ' '] # 비어 있는 문장 삭제\n",
    "\n",
    "# 문자 단위 슬라이딩 윈도우 함수\n",
    "def sliding_window_char(text, window_size=100, plus=1, sentence=True, remove_parenthesse=False):\n",
    "    if sentence == False:\n",
    "        if remove_parenthesse == True:\n",
    "            text = remove_parenthesse_func(text)\n",
    "        if len(text) < window_size:\n",
    "            return [text]\n",
    "        return [text[i:i+window_size] for i in range(0, len(text)-window_size+1, plus)]\n",
    "    else:\n",
    "        if remove_parenthesse == True:\n",
    "            text = remove_parenthesse_func(text)\n",
    "        sentences = split_sentence(text)\n",
    "        result = []\n",
    "        for text in sentences:\n",
    "            if len(text) < window_size:\n",
    "                result.extend([text])\n",
    "            else:\n",
    "                result.extend([text[i:i+window_size] for i in range(0, len(text)-window_size+1, plus)])\n",
    "        return result\n",
    "\n",
    "# 단어 단위 슬라이딩 윈도우 함수\n",
    "def sliding_window_word(text, window_size=10, plus=1, sentence=True, remove_parenthesse=False):\n",
    "    if sentence == False:\n",
    "        if remove_parenthesse == True:\n",
    "            text = remove_parenthesse_func(text)\n",
    "        words = text.split(' ')\n",
    "        if len(words) < window_size:\n",
    "            return [' '.join(words).rstrip('.')]\n",
    "        return [' '.join(words[i:i+window_size]).rstrip('.') for i in range(0, len(words)-window_size+1, plus)]\n",
    "    else:\n",
    "        if remove_parenthesse == True:\n",
    "            text = remove_parenthesse_func(text)\n",
    "        sentences = split_sentence(text)\n",
    "        result = []\n",
    "        for text in sentences:\n",
    "            words = text.split(' ')\n",
    "            if len(words) < window_size:\n",
    "                result.extend([' '.join(words).rstrip('.')])\n",
    "            else:\n",
    "                result.extend([' '.join(words[i:i+window_size]).rstrip('.') for i in range(0, len(words)-window_size+1, plus)])\n",
    "        return result\n",
    "    \n",
    "# 전처리 딕셔너리 반환 함수\n",
    "def get_preprocess_fuc_list(char_window_size : int, word_window_size : int):\n",
    "    return {\n",
    "        f\"char-{char_window_size}-sentenceO-parenthesseO\" : lambda text : sliding_window_char(text, char_window_size, sentence=True, remove_parenthesse=False),\n",
    "        f\"char-{char_window_size}-sentenceO-parenthesseX\" : lambda text : sliding_window_char(text, char_window_size, sentence=True, remove_parenthesse=True),\n",
    "        f\"char-{char_window_size}-sentenceX-parenthesseO\" : lambda text : sliding_window_char(text, char_window_size, sentence=False, remove_parenthesse=False),\n",
    "        f\"char-{char_window_size}-sentenceX-parenthesseX\" : lambda text : sliding_window_char(text, char_window_size, sentence=False, remove_parenthesse=True),\n",
    "        f\"word-{word_window_size}-sentenceO-parenthesseO\" : lambda text : sliding_window_word(text, word_window_size, sentence=True, remove_parenthesse=False),\n",
    "        f\"word-{word_window_size}-sentenceO-parenthesseX\" : lambda text : sliding_window_word(text, word_window_size, sentence=True, remove_parenthesse=True),\n",
    "        f\"word-{word_window_size}-sentenceX-parenthesseX\" : lambda text : sliding_window_word(text, word_window_size, sentence=False, remove_parenthesse=False),\n",
    "        f\"word-{word_window_size}-sentenceX-parenthesseO\" : lambda text : sliding_window_word(text, word_window_size, sentence=False, remove_parenthesse=True),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 자동화 함수 정의\n",
    "\n",
    "# 문서 전처리 자동화 함수\n",
    "def autopreprocessing(data_path = r\"./data/test_dataset\", result_path = r\"./result\", char_window_size = 100, word_window_size = 10):\n",
    "    os.makedirs(f\"{result_path}/preprocess\", exist_ok=True)\n",
    "    text_file_list = [f for f in os.listdir(data_path) if f.endswith(\".txt\")]\n",
    "    # 전처리 함수와 이름 정의\n",
    "    preprocess_fuc_list = get_preprocess_fuc_list(char_window_size, word_window_size)\n",
    "\n",
    "    # 각 실험 폴더 생성\n",
    "    for filepath in preprocess_fuc_list.keys():\n",
    "        os.makedirs(f\"{result_path}/preprocess/{filepath}\", exist_ok=True)\n",
    "    \n",
    "    # 전처리 데이터 저장\n",
    "    for textfile in tqdm(text_file_list, desc=\"Preprocessing : \", ncols=100):\n",
    "        with open(f\"{data_path}/{textfile}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "        for key, fucn in preprocess_fuc_list.items():\n",
    "            text_preprocess = fucn(text)\n",
    "            with open(f\"{result_path}/preprocess/{key}/{textfile}.pkl\", \"wb\") as f:\n",
    "                pickle.dump(text_preprocess, f)\n",
    "\n",
    "# 전처리 문서 임베딩 자동화화 함수\n",
    "def autoencode(result_path = r\"./result\", model = SentenceTransformer(\"jhgan/ko-sbert-sts\")):\n",
    "    path_list = os.listdir(f\"{result_path}/preprocess\")\n",
    "    for path in tqdm(path_list, desc=\"Encoding : \", ncols=100):\n",
    "        os.makedirs(f\"{result_path}/encode/{path}\", exist_ok=True)\n",
    "        pkl_list = [f for f in os.listdir(f\"{result_path}/preprocess/{path}\") if f.endswith(\".pkl\")]\n",
    "        for pkl_file in pkl_list:\n",
    "            with open(f\"{result_path}/preprocess/{path}/{pkl_file}\", \"rb\") as f2:\n",
    "                encode_list = model.encode(pickle.load(f2), normalize_embeddings=True)\n",
    "            with open(f\"{result_path}/encode/{path}/{pkl_file}\", \"wb\") as f2:\n",
    "                pickle.dump(encode_list, f2)\n",
    "\n",
    "# 문서 간 유사도 비교 자동화 함수\n",
    "def autocheck(result_path = r\"./result\", threshold_1 = 0.85, threshold_2 = 0.9):\n",
    "    path_list = os.listdir(f\"{result_path}/encode\")\n",
    "    os.makedirs(f\"{result_path}/result/{threshold_1}-{threshold_2}\", exist_ok=True)\n",
    "    csv_path = f\"{result_path}/result/{threshold_1}-{threshold_2}/result.csv\"\n",
    "    first_write = not os.path.exists(csv_path)\n",
    "    for path in tqdm(path_list, \"Checking : \", ncols=100):\n",
    "        pkl_list = [f for f in os.listdir(f\"{result_path}/encode/{path}\") if f.endswith(\".pkl\")]\n",
    "        for i in range(len(pkl_list)):\n",
    "            with open(f\"{result_path}/encode/{path}/{pkl_list[i]}\", \"rb\") as f1:\n",
    "                pivot_data = np.array(pickle.load(f1))\n",
    "            for j in range(i+1, len(pkl_list)):\n",
    "                with open(f\"{result_path}/encode/{path}/{pkl_list[j]}\", \"rb\") as f2:\n",
    "                    compare_data = np.array(pickle.load(f2))\n",
    "                cnt = 0\n",
    "                for pivot_list in pivot_data:\n",
    "                    for compare_list in compare_data:\n",
    "                        sim = util.cos_sim(pivot_list, compare_list)\n",
    "                        if sim >= threshold_1:\n",
    "                            cnt += 1\n",
    "                            break\n",
    "                label = 1 if cnt >= len(pivot_data) * threshold_2 else 0\n",
    "\n",
    "                row = pd.DataFrame([{\n",
    "                    \"type\": path,\n",
    "                    \"original\": pkl_list[i],\n",
    "                    \"compare\": pkl_list[j],\n",
    "                    \"similarity\": cnt,\n",
    "                    \"label\": label,\n",
    "                }])\n",
    "\n",
    "                row.to_csv(csv_path, mode=\"a\", header=first_write, index=False)\n",
    "                first_write = False\n",
    "                return csv_path\n",
    "\n",
    "# 전체 자동 실험 함수\n",
    "def autorun(data_path = r\"./data/test_dataset\", result_path = r\"./result\", char_window_size = 100, word_window_size = 10, model = SentenceTransformer(\"jhgan/ko-sbert-sts\"), threshold_1 = 0.85, threshold_2 = 0.90): # Model은 일단 유지\n",
    "    autopreprocessing(data_path, result_path, char_window_size, word_window_size)\n",
    "    print(\"문서 전처리 완료\")\n",
    "    autoencode(result_path, model)\n",
    "    print(\"문서 임베딩 완료\")\n",
    "    csv_path = autocheck(result_path, threshold_1, threshold_2)\n",
    "    print(f\"완료되었습니다. 결과가 {csv_path} 에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 실행\n",
    "autorun(data_path=r\"../data/test_dataset\", result_path=r\"./result\", char_window_size=100, word_window_size=10, threshold_1=0.85, threshold_2=0.90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Capstone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
