import os
import streamlit as st
from langchain.prompts import ChatPromptTemplate
from langchain.document_loaders import UnstructuredFileLoader
from langchain.embeddings import HuggingFaceEmbeddings 
from langchain.schema.runnable import RunnablePassthrough
from langchain.storage import LocalFileStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain.schema import ChatMessage
from langserve import RemoteRunnable
from sentence_transformers import SentenceTransformer


st.set_page_config(page_title="Ollama Local ëª¨ë¸ í…ŒìŠ¤íŠ¸", page_icon="ğŸ’¬")
st.title("Ollama local ëª¨ë¸")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

@st.cache_resource(show_spinner="Embedding file...")
def embed_file(file):
    file_content = file.read()
    
    file_dir = "./.cache/files/"
    file_path = os.path.join(file_dir, file.name)

    embedding_dir = "./.cache/embeddings/"

    os.makedirs(file_dir, exist_ok=True)
    os.makedirs(embedding_dir, exist_ok=True)

    with open(file_path, "wb") as f:
        f.write(file_content)

    cache_dir = LocalFileStore(os.path.join(embedding_dir, file.name))

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "(?<=\.)", " ", ""],
        length_function=len,
    )

    loader = UnstructuredFileLoader(file_path)
    docs = loader.load_and_split(text_splitter=text_splitter)


    model_path = "C:/Users/seclab/Dev/Langchain-ollama/model/ko-sbert-sts"

    if os.path.exists(model_path):
        print(f"âœ… ê²½ë¡œê°€ ì¡´ì¬í•©ë‹ˆë‹¤: {model_path}")

        try:
            embeddings = HuggingFaceEmbeddings(model_name=model_path)
            embedding_model = SentenceTransformer(model_path)
        
            test_sentence = "ì•ˆë…•í•˜ì„¸ìš”, í…ŒìŠ¤íŠ¸ ë¬¸ì¥ì…ë‹ˆë‹¤."
            test_vector = embedding_model.encode(test_sentence)  

            print("âœ… ëª¨ë¸ì´ ì •ìƒì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
            print(f"ğŸ”¹ í…ŒìŠ¤íŠ¸ ë¬¸ì¥ ì„ë² ë”© ë²¡í„° í¬ê¸°: {test_vector.shape}")

        except Exception as e:
            print("âŒ ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ!")
            print(e)
    else:
        print(f"âŒ ëª¨ë¸ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")


    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()
    return retriever

with st.sidebar:
    file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf", "txt", "docx"])

if file:
    retriever = embed_file(file)

print_history()

if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)
    with st.chat_message("assistant"):
        ollama = RemoteRunnable("http://localhost:8000/chat/")

        with st.spinner("ë‹µë³€ì„ ìƒê°í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                if file is not None and any(keyword in user_input for keyword in ["íŒŒì¼", "ë¬¸ì„œ", "ë‚´ìš©", "ì„¤ëª…", "ì •ë³´"]):
                    prompt = ChatPromptTemplate.from_template(
                        "ë‹¤ìŒ ë¬¸ë§¥ì„ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”:\n\n{context}\n\nì§ˆë¬¸: {question}"
                    )
                    rag_chain = (
                        {
                            "context": retriever | (lambda docs: "\n\n".join(doc.page_content for doc in docs)),
                            "question": RunnablePassthrough(),
                        }
                        | prompt
                        | ollama
                        | StrOutputParser()
                    )
                    answer = rag_chain.invoke(user_input)
                else:
                    prompt = ChatPromptTemplate.from_template("ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:\n{input}")
                    chain = prompt | ollama | StrOutputParser()
                    answer = chain.invoke(user_input)

                add_history("ai", answer)
                st.write(answer)
            except Exception as e:
                st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")