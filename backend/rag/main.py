import os
import streamlit as st
from langchain.prompts import ChatPromptTemplate
from langchain_unstructured import UnstructuredLoader
from langchain.embeddings import CacheBackedEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema.runnable import RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain.storage import LocalFileStore
from langchain.schema import ChatMessage
from langserve import RemoteRunnable
from langchain import hub

# ì‹¤í–‰í•˜ë©´ ì²˜ìŒ ëœ¨ëŠ” í˜ì´ì§€ íƒ€ì´í‹€
st.set_page_config(page_title="Ollama Local ëª¨ë¸ í…ŒìŠ¤íŠ¸", page_icon="ğŸ’¬")
st.title("Ollama local ëª¨ë¸")

# ì‹œì‘í•˜ë©´ì„œ chatbotì´ "ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"ë¼ë©° ì±„íŒ…ì„ ë„ì›ë‹ˆë‹¤
if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]


# ì§€ê¸ˆê¹Œì§€ ì£¼ê³ ë°›ì€ ëª¨ë“  ë©”ì‹œì§€ë¥¼ Streamlit ì±„íŒ… UIì— ì¶œë ¥í•˜ëŠ” ì½”ë“œ
def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)


# ìƒˆë¡œìš´ ë©”ì‹œì§€ë¥¼ ì±„íŒ… ê¸°ë¡(session_state)ì— ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))

 
# ê²€ìƒ‰í•œ ë¬¸ì„œ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¬¸ë‹¨ìœ¼ë¡œ í•©ì³ì¤ë‹ˆë‹¤
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# íŒŒì¼ì„ ì˜¬ë¦¬ë©´ Embedding file...ì´ë¼ ëœ¹ë‹ˆë‹¤
@st.cache_resource(show_spinner="Embedding file...")


# ì˜¬ë¦° íŒŒì¼ì„ ì„ë² ë”©í•˜ëŠ” ì½”ë“œ
def embed_file(file):
    file_content = file.read()

    # íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì •
    file_path = f"./.cache/files/{file.name}"
    
    # ì½ì€ íŒŒì¼ì„ ì„ì‹œ ìºì‹œì— ì €ì¥
    with open(file_path, "wb") as f:
        f.write(file_content)

    # ì„ë°°ë”© ìºì‹œë¥¼ ì €ì¥í•  ë””ë ‰í† ë¦¬ ì„¤ì •
    cache_dir = LocalFileStore(f"./.cache/embeddings/{file.name}")

    # í…ìŠ¤íŠ¸ ë¶„í• ê¸° ì„¤ì • : ë¬¸ì„œë¥¼ ì‘ì€ ì²­í¬ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "(?<=\.)", " ", ""],
        length_function=len,
    )

    # pdf, docx, txt íŒŒì¼ ë¡œë”
    loader = UnstructuredLoader(file_path)

    # ë¬¸ì„œë¥¼ ë¶„í• í•˜ì—¬ docs ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“­ë‹ˆë‹¤
    docs = loader.load_and_split(text_splitter=text_splitter)

    # Sbert ì„ë² ë”© ëª¨ë¸ ì§€ì •
    model_name = "jhgan/ko-sbert-sts"
    embeddings = HuggingFaceEmbeddings(model_name=model_name)
    
    # ì„ë² ë”© ê°ì²´ ìƒì„± (ì†ë„ë¥¼ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•¨ì…ë‹ˆë‹¤)
    cached_embeddings = CacheBackedEmbeddings.from_bytes_store(embeddings, cache_dir)

    # ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë²¡í„°í™” í•œ í›„ì— FAISS ë²¡í„° DBì— ì €ì¥í•©ë‹ˆë‹¤
    vectorstore = FAISS.from_documents(docs, cached_embeddings)
    
    # ê²€ìƒ‰ê¸° í˜•íƒœë¡œ ë°˜í™˜í•˜ì—¬ RAG ê¸°ëŠ¥ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤
    retriever = vectorstore.as_retriever()
    return retriever

with st.sidebar:
    file = st.file_uploader(
        "íŒŒì¼ ì—…ë¡œë“œ", 
        type=["pdf", "txt", "docx"],
        )

# íŒŒì¼ì„ ì˜¬ë ¸ì„ ê²½ìš° retrieverë¥¼ ë§Œë“¤ì–´ RAG ê¸°ëŠ¥ì„ êµ¬í˜„í•©ë‹ˆë‹¤
if file:
    retriever = embed_file(file)

print_history()

# ì‚¬ìš©ìê°€ ì±„íŒ…ì„ ì…ë ¥í–ˆë‹¤ë©´
if user_input := st.chat_input():

    # 1. ì…ë ¥í•œ ë‚´ìš©ì„ ì„¸ì…˜ì— ì €ì¥í•©ë‹ˆë‹¤
    add_history("user", user_input)

    # 2. í™”ë©´ì— ì‚¬ìš©ì ë©”ì„¸ì§€ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤
    st.chat_message("user").write(user_input)

    # 3. chatbotì˜ ì‘ë‹µ ì‹œì‘
    with st.chat_message("assistant"):

        # ë¡œì»¬ì—ì„œ ëŒë¦¬ëŠ” Ollama Langserverì— ì—°ê²°
        ollama = RemoteRunnable("http://localhost:8000/chat")

        with st.spinner("ë‹µë³€ì„ ìƒê°í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            if file is not None:
                # Langchainì´ ì œê³µí•˜ëŠ” ragìš© prompt -> ì´ê±´ chat.pyì— ragìš© promptë¥¼ ì§ì ‘ ë§Œë“¤ê¹Œ ê³ ë¯¼ì¤‘...
                prompt = hub.pull("rlm/rag-prompt")

                # Rag ì²´ì¸ ì •ì˜
                rag_chain = (
                    {
                        "context": retriever | format_docs,  # ë¬¸ì„ ê²€ìƒ‰ + í…ìŠ¤íŠ¸ í¬ë§·
                        "question": RunnablePassthrough()    # ì§ˆë¬¸ ì›ë³¸ ì „ë‹¬
                    }
                    | prompt    # Rag promptë¥¼
                    | ollama    # ollama ëª¨ë¸ì— ì „ë‹¬
                    | StrOutputParser() # ëª¨ë¸ ì‘ë‹µì„ ë¬¸ìì—´ë¡œ parsing
                )

                # ë¬¸ì„œì— ëŒ€í•œ ì§ˆì˜ë¥¼ ì…ë ¥í•˜ê³ , ë‹µë³€ì„ ì¶œë ¥í•˜ë¹ˆë‹¤.
                answer = rag_chain.invoke(
                    user_input
                )

                add_history("ai", answer)
            else:
                # rag ì•ˆ ì¼ì„ ê²½ìš°(ë¬¸ì„œ ì—†ì„ ê²½ìš°)
                
                prompt = ChatPromptTemplate.from_template(
                    "ë‹¤ìŒì˜ ì§ˆë¬¸ì— ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”:\n{input}"
                )

                # ì²´ì¸ ìƒì„±
                chain = prompt | ollama | StrOutputParser()

                answer = chain.invoke(user_input)
                add_history("ai", answer)
        
        # ë‹µë³€ ì¶œë ¥
        st.write(answer)