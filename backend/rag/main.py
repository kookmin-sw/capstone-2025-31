import os
import streamlit as st
from langchain.prompts import ChatPromptTemplate
from langchain_unstructured import UnstructuredLoader
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.schema.runnable import RunnablePassthrough
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.output_parsers import StrOutputParser
from langchain_core.messages import HumanMessage
from langchain.schema import ChatMessage
from langserve import RemoteRunnable
from chat import chain as chat_chain, rag_prompt

st.set_page_config(page_title="Ollama Local ëª¨ë¸ í…ŒìŠ¤íŠ¸", page_icon="ğŸ’¬")
st.title("Ollama local ëª¨ë¸")

if "messages" not in st.session_state:
    st.session_state["messages"] = [
        ChatMessage(role="assistant", content="ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?")
    ]

def print_history():
    for msg in st.session_state.messages:
        st.chat_message(msg.role).write(msg.content)

def add_history(role, content):
    st.session_state.messages.append(ChatMessage(role=role, content=content))

def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

@st.cache_resource(show_spinner="Embedding file...")

# ì˜¬ë¦° íŒŒì¼ì„ ì„ë² ë”©í•˜ëŠ” ì½”ë“œ
def embed_file(file):
    file_content = file.read()
    
    # ì˜¬ë¦° íŒŒì¼ì„ ì €ì¥í•  ê³µê°„(í•´ë‹¹ íŒŒì¼ì— ìë™ ìƒì„±)
    file_dir = "./.cache/files/"
    file_path = os.path.join(file_dir, file.name)

    embedding_dir = "./.cache/embeddings/"

    os.makedirs(file_dir, exist_ok=True)
    os.makedirs(embedding_dir, exist_ok=True)

    with open(file_path, "wb") as f:
        f.write(file_content)

    # ì„ì˜ë¡œ ì„¤ì •í•œ split sizeì…ë‹ˆë‹¤. 
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,
        chunk_overlap=50,
        separators=["\n\n", "\n", "(?<=\.)", " ", ""],
        length_function=len,
    )

    # pdf íŒŒì¼ ë¡œë”
    loader = UnstructuredLoader(file_path)
    docs = loader.load_and_split(text_splitter=text_splitter)


    model_path = "C:/Users/seclab/Dev/Langchain-ollama/model/ko-sbert-sts"
    embeddings = HuggingFaceEmbeddings(model_name=model_path)

    # ë‹¤ë¥¸ ë¶„ë“¤ì€ ì´ ì½”ë“œë¡œ í¸ì•ˆíˆ ì„ë² ë”© í•˜ì‹œë©´ ë©ë‹ˆë‹¤
    # model_path = "jhgan/ko-sbert-sts"
    # embeddings = HuggingFaceEmbeddings(model_name=model_path)
    
    vectorstore = FAISS.from_documents(docs, embeddings)
    retriever = vectorstore.as_retriever()
    return retriever

with st.sidebar:
    file = st.file_uploader("íŒŒì¼ ì—…ë¡œë“œ", type=["pdf", "txt", "docx"])

if file:
    retriever = embed_file(file)

print_history()

if user_input := st.chat_input():
    add_history("user", user_input)
    st.chat_message("user").write(user_input)

    with st.chat_message("assistant"):
        ollama = RemoteRunnable("http://localhost:8000/chat/")


        with st.spinner("ë‹µë³€ì„ ìƒê°í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                if file is not None and any(keyword in user_input for keyword in ["íŒŒì¼", "ë¬¸ì„œ", "ë‚´ìš©", "ì„¤ëª…", "ì •ë³´"]):
                    rag_chain = (
                        {
                            "context": retriever | (lambda docs: "\n\n".join(doc.page_content for doc in docs)),
                            "question": RunnablePassthrough(),
                        }
                        # chat.pyì—ì„œ ê°€ì ¸ì˜¨ ragì˜ prompt
                        | rag_prompt
                        | ollama
                        | StrOutputParser()
                    )
                    answer = rag_chain.invoke(user_input)
                else:
                    # ragê¸°ëŠ¥ì´ í•„ìš” ì—†ëŠ” ê²½ìš°ì—” chat.pyì˜ ê¸°ì¡´ í”„ë¡¬í”„íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì´ìš©
                    answer = chat_chain.invoke({"messages": [HumanMessage(content=user_input)]})

                add_history("ai", answer)
                st.write(answer)

            except Exception as e:
                st.error(f"ì‘ë‹µ ìƒì„± ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")